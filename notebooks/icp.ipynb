{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import read_canonical_model, load_pc\n",
    "# import autograd.numpy as np\n",
    "#import utils\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import os\n",
    "os.chdir(\"/Users/mustafa/Documents/grad school/spring 2024/ECE285 - Visual Learning/PointCloudPosePrediction\")\n",
    "from utils.utils import invert_pose\n",
    "import importlib\n",
    "import yaml\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial import KDTree\n",
    "import transforms3d as t3d\n",
    "from transforms3d.quaternions import mat2quat\n",
    "import transforms3d.quaternions as tq\n",
    "import transforms3d.axangles as ax\n",
    "# from autograd import grad\n",
    "from numpy.linalg import norm\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import dgl.geometry\n",
    "import numpy as np\n",
    "# from .utils import read_canonical_model, load_pc, visualize_icp_result\n",
    "# from icp_file import icp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('TkAgg')\n",
    "import torch\n",
    "import core.pointnet as pnet\n",
    "importlib.reload(pnet)\n",
    "import utils.dataloader\n",
    "import utils.utils\n",
    "import utils.vision\n",
    "importlib.reload(utils.utils)\n",
    "importlib.reload(utils.vision)\n",
    "importlib.reload(utils.dataloader)\n",
    "import scipy.io as sio\n",
    "import open3d as o3d\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= utils.dataloader.EdenDataset(\"/Users/mustafa/Documents/grad school/spring 2024/ECE285 - Visual Learning/eden/sample\",\n",
    "#                                             train=True,\n",
    "                                            use_keypt_downsample = True,\n",
    "                                            keypt_method=\"random\"#params['keypoint_detection']['detection_method']\n",
    "                                           #pcd_num_pts=60000# pcd_num_pts=params['keypoint_detection']['pcd_num_pts']\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_icp_result(source_pc, target_pc, pose):\n",
    "  '''\n",
    "  Visualize the result of ICP\n",
    "  source_pc: numpy array, (N, 3)\n",
    "  target_pc: numpy array, (N, 3)\n",
    "  pose: SE(4) numpy array, (4, 4)\n",
    "  '''\n",
    "  source_pcd = o3d.geometry.PointCloud()\n",
    "  source_pcd.points = o3d.utility.Vector3dVector(source_pc.reshape(-1, 3))\n",
    "  source_pcd.paint_uniform_color([0, 1, 1])\n",
    "\n",
    "  target_pcd = o3d.geometry.PointCloud()\n",
    "  target_pcd.points = o3d.utility.Vector3dVector(target_pc.reshape(-1, 3))\n",
    "  target_pcd.paint_uniform_color([1, 0, 0])\n",
    "\n",
    "  source_pcd.transform(pose)\n",
    "  #target_pcd.transform(np.linalg.inv(pose))\n",
    "\n",
    "  o3d.visualization.draw_geometries([source_pcd, target_pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_pose = dataset[55]['rel_pose']\n",
    "# new_pose[:3, 3] = dataset[55]['rel_pose'][:3, 3] \n",
    "# visualize_icp_result(dataset[55]['pcd1'], dataset[55]['pcd2'], invert_pose(new_pose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "# visualize the trajectory of a sequence\n",
    "\n",
    "batch_size = 99 # number of times stamps\n",
    "iters = 100\n",
    "positions = np.zeros((batch_size, 3))\n",
    "poses = np.zeros((4, 4, batch_size))\n",
    "poses[:, :, 0] = invert_pose(dataset[0]['pose']) # Pose of the robot in body frame at time t = 0\n",
    "poses[:, :, 0][:3, 3] = poses[:, :, 0][:3, 3] * 100\n",
    "\n",
    "\n",
    "error_p = np.zeros(batch_size)\n",
    "error_R = np.zeros(batch_size)\n",
    "\n",
    "# Pass xyz to Open3D.o3d.geometry.PointCloud and visualize\n",
    "\n",
    "source = o3d.geometry.PointCloud()\n",
    "target = o3d.geometry.PointCloud()\n",
    "\n",
    "threshold = 0.02 # thresholf for ICP\n",
    "for i in range(batch_size - 1):\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "\n",
    "    positions[i] = invert_pose(dataset[i]['pose'])[:3,3] # ground truth poses \n",
    "\n",
    "    source.points = o3d.utility.Vector3dVector(dataset[i]['pcd1'].numpy())\n",
    "    target.points = o3d.utility.Vector3dVector(dataset[i]['pcd2'].numpy())\n",
    "\n",
    "\n",
    "    if i < 5 or i % 20 == 0: # at the beginning I only have ground truth rel pose\n",
    "        trans_init = dataset[i]['rel_pose'].numpy()\n",
    "#         trans_init[:3, 3] = dataset[i]['rel_pose'][:3, 3].numpy() * 100\n",
    "#         trans_init = np.eye(4)\n",
    "        trans_init[0, 3] =  dataset[i]['rel_pose'][0, 3].numpy() * 100\n",
    "        trans_init[1, 3] =  dataset[i]['rel_pose'][1, 3].numpy() * 100\n",
    "\n",
    "    else:\n",
    "        trans_init = deepcopy(reg_p2p.transformation) # assumes model continuity , initialize ICP with prev rel pose\n",
    "#         print(trans_init)\n",
    "#         trans_init[:3, 3] = trans_init[:3, 3] * 100\n",
    "#         trans_init = dataset[i]['rel_pose'].numpy() * 100\n",
    "\n",
    "    \n",
    "    reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "    source, target, threshold, trans_init,\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPoint()) # run ICPs\n",
    "#     print(reg_p2p.transformation)\n",
    "        \n",
    "    \n",
    "#     source = np.transpose(dataset[i]['pose'])[0:3, 0:3].numpy() @ (dataset[i]['pcd1'].numpy() - dataset[i]['pose'][:3, 3].numpy()).T\n",
    "#     target = np.transpose(dataset[i + 1]['pose'])[0:3, 0:3].numpy() @ (dataset[i]['pcd2'].numpy() - dataset[i + 1]['pose'][:3, 3].numpy()).T\n",
    "    \n",
    "    # initialize\n",
    "#     p_initial = dataset[i]['translation'].numpy() \n",
    "#     R_initial = R.from_quat(dataset[i]['quat']) # Rotation between t+1 and t\n",
    "    \n",
    "    #pose_initial = icp(5, target, source, np.eye(3), p_initial) # get initial rotation\n",
    "    \n",
    "    # Run ICP to get the pose at the next time step t_T_t+1\n",
    "#     rel_pose, _ = icp(iters, target, source, dataset[i]['rel_pose'][0:3, 0:3], dataset[i]['rel_pose'][:3, 3])\n",
    "    \n",
    "#     R_rel = rel_pose[0:3, 0:3] # relative rotation from icp\n",
    "#     p_rel = rel_pose[:3, 3] # relative translation from icp\n",
    "    \n",
    "#     error_p[i] = np.linalg.norm(p_rel - dataset[i]['rel_pose'].numpy()[:3, 3])\n",
    "#     error_R[i] = ax.mat2axangle(np.matmul(R_rel, dataset[i]['rel_pose'][0:3, 0:3]))[1]\n",
    "\n",
    "        \n",
    "#     print(\"Initialization angle to ICP: \", ax.mat2axangle(trans_init[0:3, 0:3])[1])\n",
    "#     print(\"ICP output angle\", ax.mat2axangle(reg_p2p.transformation[0:3, 0:3])[1])\n",
    "    \n",
    "#     print(\"    \")\n",
    "# #     print(\"Initialization angle to ICP: \", ax.mat2axangle(trans_init[0:3, 0:3])[1])\n",
    "#     print(\"Translation initialization to ICP: \", trans_init[:3, 3])\n",
    "    \n",
    "# #     print(\"ICP output angle\", ax.mat2axangle(reg_p2p.transformation[0:3, 0:3])[1])\n",
    "#     print(\"ICP translation output: \", reg_p2p.transformation[:3, 3])\n",
    "    \n",
    "#     print(\"\")\n",
    "    \n",
    "#     if i == 50:\n",
    "    \n",
    "#         break\n",
    "\n",
    "    #estimate pose at the next time step\n",
    "    poses[:, :, i + 1] = np.matmul(reg_p2p.transformation, poses[:, :, i]) # world to body pose\n",
    "#     poses[:, :, i + 1] = np.matmul(poses[:, :, i], reg_p2p.transformation) # world to body pose\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Update\n",
    "#     prev_rel_pose = reg_p2p.transformation # relative pose estimated by ICP\n",
    "    \n",
    "\n",
    "# Invert poses for plotting purposes\n",
    "# for i in range(64):\n",
    "#     poses[:, :, i] = invert_pose(poses[:, :, i])\n",
    "\n",
    "plt.plot(positions[:batch_size-1,0] * 100, positions[:batch_size-1,1] * 100)\n",
    "plt.plot(poses[0, -1, :], poses[1, -1, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for evo visualization\n",
    "\n",
    "mat_est = np.zeros((batch_size, 8))\n",
    "mat_gt = np.zeros((batch_size, 8))\n",
    "for i in range(batch_size):\n",
    "    \n",
    "    # quat from poses pred\n",
    "    new_quat = np.zeros((4))\n",
    "    quat = mat2quat(poses[:, :, i][0:3, 0:3])\n",
    "    new_quat[0] = quat[1]\n",
    "    new_quat[1] = quat[2]\n",
    "    new_quat[2] = quat[3]\n",
    "    new_quat[3] = quat[0]\n",
    "\n",
    "\n",
    "    mat_est[i, :] = np.append(i, np.append(poses[:, :, i][:3, 3], new_quat))\n",
    "    mat_gt[i, :] = np.append(i, np.append(invert_pose(dataset[i]['pose'])[:3, 3] * 100, mat2quat(invert_pose(dataset[i]['pose'])[0:3, 0:3])))\n",
    "\n",
    "\n",
    "filename_est = './results/stamped_traj_estimate_icp.txt'\n",
    "filename_gt = './results/stamped_groundtruth_icp.txt'\n",
    "\n",
    "# Save the matrix to the text file with the desired format\n",
    "np.savetxt(filename_est, mat_est, fmt='%.10e', delimiter=' ')\n",
    "np.savetxt(filename_gt, mat_gt, fmt='%.10e', delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run evo_traj for plots for trajectory\n",
    "# evo_traj tum results/plotting_test/stamped_traj_estimate.txt --ref=rpg_trajectory_evaluation/results/plotting_test/stamped_groundtruth.txt -p --plot_mode=xy\n",
    "# run evo_ape to output data for plotting later\n",
    "# evo_ape tum ./stamped_groundtruth.txt ./stamped_traj_estimate.txt -va --plot --plot_mode xy --save_results ./evo_ape.zip\n",
    "# run evo_res for tables, histograms\n",
    "# evo_res evo_ape.zip -p --save_table ./table.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
