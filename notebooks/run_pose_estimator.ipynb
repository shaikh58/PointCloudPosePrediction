{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32d2c005-96c3-4d67-824f-dddd483d7bd7",
   "metadata": {},
   "source": [
    "### Set the run number here; decides which directory to save model and results - should match the number on the corresponding yaml file for the model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8edeebc-4373-4543-ad27-94bcbd3dda8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_num = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3d3acdc-da2a-4e9e-b5f1-20b08447eeac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.dataloader' from '/Users/mustafa/Documents/grad school/spring 2024/ECE285 - Visual Learning/PointCloudPosePrediction/utils/dataloader.py'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "import os\n",
    "# curr_dir = \"/home/mushaikh/private/PointCloudPosePrediction\"\n",
    "curr_dir = \"/Users/mustafa/Documents/grad school/spring 2024/ECE285 - Visual Learning/PointCloudPosePrediction\"\n",
    "os.chdir(curr_dir)\n",
    "import importlib\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import open3d as o3d\n",
    "from scipy.io import loadmat\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Callable, Optional, Tuple\n",
    "import yaml\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import core.pointnet as pnet\n",
    "import core.deep_pose_estimator as dpe\n",
    "import core.loss as dpe_loss\n",
    "import utils.dataloader\n",
    "import utils.utils\n",
    "import utils.vision\n",
    "importlib.reload(dpe)\n",
    "importlib.reload(pnet)\n",
    "importlib.reload(utils.utils)\n",
    "importlib.reload(utils.vision)\n",
    "importlib.reload(utils.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d98a88de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "else:\n",
    "    torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "608fd5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load params for current model run; choose model run below after creating yaml file for it\n",
    "with open(\"./models/run_{x}/run_{x}.yaml\".format(x=run_num)) as f:\n",
    "    params = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "732ecfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader for Eden data - outputs pairs of point clouds and relative poses\n",
    "batch_size = params['train']['batch_size']\n",
    "# \"/home/mushaikh/private/sample\"\n",
    "dataset = utils.dataloader.EdenDataset(\"/Users/mustafa/Documents/grad school/spring 2024/ECE285 - Visual Learning/eden/sample\",\n",
    "                                            keypt_method=params['keypoint_detection']['detection_method'],\n",
    "                                            pcd_num_pts = params['keypoint_detection']['pcd_num_pts']\n",
    "                                            )\n",
    "\n",
    "train_inds, val_inds = torch.utils.data.random_split(dataset, [0.8,0.2])\n",
    "\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(train_inds.indices)\n",
    "valid_sampler = torch.utils.data.SubsetRandomSampler(val_inds.indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10626ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [01:45<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# get preds from model\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m pred, trans_feat_1, trans_feat_2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcd1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpcd2\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# predicts 7-length vector of translation/quaternion\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# compute loss\u001b[39;00m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m dpe_loss\u001b[38;5;241m.\u001b[39mPoseLoss(pred, target) \n",
      "File \u001b[0;32m~/miniconda3/envs/ece276/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/grad school/spring 2024/ECE285 - Visual Learning/PointCloudPosePrediction/core/deep_pose_estimator.py:81\u001b[0m, in \u001b[0;36mDeepPoseEstimator.forward\u001b[0;34m(self, pcd1, pcd2)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pcd1, pcd2):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# input pcd : batch_size x 3 x num_pts (transposed before input)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     pcd1_feat, trans_feat_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpcd_feat(pcd1)\n\u001b[0;32m---> 81\u001b[0m     pcd2_feat, trans_feat_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpcd_feat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcd2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# after feature extraction : batch_size x num_pts x 128\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     pcd1_feat, pcd2_feat \u001b[38;5;241m=\u001b[39m pcd1_feat\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)), pcd2_feat\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/ece276/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/grad school/spring 2024/ECE285 - Visual Learning/PointCloudPosePrediction/core/pointnet.py:197\u001b[0m, in \u001b[0;36mPointNetFeatureExtractor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    195\u001b[0m n_pts \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    196\u001b[0m x, trans, trans_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat(x)\n\u001b[0;32m--> 197\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    198\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m    199\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)))\n",
      "File \u001b[0;32m~/miniconda3/envs/ece276/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ece276/lib/python3.9/site-packages/torch/nn/modules/conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ece276/lib/python3.9/site-packages/torch/nn/modules/conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    307\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    308\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "model = dpe.DeepPoseEstimator(num_pts=params['keypoint_detection']['pcd_num_pts'], \n",
    "                              use_pca = params['model']['use_pca'])\n",
    "model.to(device)\n",
    "lr = params['train']['lr']\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "list_train_loss = []\n",
    "list_val_loss = []\n",
    "\n",
    "for epoch in range(params['train']['epochs']):\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    running_train_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(tqdm(train_loader)):\n",
    "        \n",
    "        # prepare data\n",
    "        pcd1, pcd2, quat, translation = data[\"pcd1\"], data[\"pcd2\"], data[\"quat\"], data[\"translation\"]\n",
    "        pcd1, pcd2 = pcd1.permute(0,2,1), pcd2.permute(0,2,1)\n",
    "        target = torch.cat((translation.transpose(1,0), quat.transpose(1,0))).transpose(1,0)\n",
    "        \n",
    "        # move to gpu\n",
    "        pcd1, pcd2, quat, translation, target = pcd1.to(device), pcd2.to(device), quat.to(device), translation.to(device), target.to(device)\n",
    "        \n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # set mode to train - sets batchnorm/dropout policy etc.\n",
    "        model.train()\n",
    "        \n",
    "        # get preds from model\n",
    "        pred, trans_feat_1, trans_feat_2 = model(pcd1, pcd2) # predicts 7-length vector of translation/quaternion\n",
    "        \n",
    "        # compute loss\n",
    "        loss = dpe_loss.PoseLoss(pred, target) \n",
    "#         + \\\n",
    "#             pnet.feature_transform_regularizer(trans_feat_1) * 0.001 + \\\n",
    "#                 pnet.feature_transform_regularizer(trans_feat_1) * 0.001\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient descent step \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = running_train_loss / (i + 1)\n",
    "    print('Avg. train loss: %f' % (avg_train_loss))\n",
    "    list_train_loss.append(avg_train_loss)\n",
    "    \n",
    "    # update the learning rate scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # check validation loss after every epoch\n",
    "    \n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization\n",
    "    model.eval()\n",
    "    running_vloss = 0.0\n",
    "    \n",
    "    # Disable gradient computation and reduce memory consumption\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            pcd1, pcd2, quat, translation = vdata[\"pcd1\"], vdata[\"pcd2\"], vdata[\"quat\"], vdata[\"translation\"]\n",
    "            pcd1, pcd2 = pcd1.permute(0,2,1), pcd2.permute(0,2,1)\n",
    "            target = torch.cat((translation.transpose(1,0), quat.transpose(1,0))).transpose(1,0)\n",
    "            pcd1, pcd2, quat, translation, target = pcd1.to(device), pcd2.to(device), quat.to(device), translation.to(device), target.to(device)\n",
    "            pred, trans_feat_1, trans_feat_2 = model(pcd1, pcd2)\n",
    "            vloss = dpe_loss.PoseLoss(pred, target) \n",
    "#             + \\\n",
    "#                 pnet.feature_transform_regularizer(trans_feat_1) * 0.001 + \\\n",
    "#                 pnet.feature_transform_regularizer(trans_feat_1) * 0.001\n",
    "            running_vloss += vloss.item()\n",
    "            \n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('Avg. validation loss: {}'.format(avg_vloss))\n",
    "    list_val_loss.append(avg_vloss)\n",
    "    \n",
    "    # save the model after every epoch\n",
    "    torch.save(model.state_dict(), '{y}/run_{x}/run_{x}_deep_pose_{z}.pth'.format(\n",
    "        x=run_num, y=os.path.join(curr_dir,\"models\"), z=epoch))\n",
    "\n",
    "    \n",
    "# plot the loss curves\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(range(params['train']['epochs']), list_train_loss, label=\"train loss\")\n",
    "ax.plot(range(params['train']['epochs']), list_val_loss, label=\"validation loss\")\n",
    "ax.set_xlabel(\"Epoch #\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "fig.legend()\n",
    "fig.savefig(os.path.join(curr_dir,\"results\",\"run_{x}_loss.png\".format(x=run_num)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
