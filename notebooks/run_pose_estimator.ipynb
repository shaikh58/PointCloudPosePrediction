{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32d2c005-96c3-4d67-824f-dddd483d7bd7",
   "metadata": {},
   "source": [
    "### Set the run number here; decides which directory to save model and results - should match the number on the corresponding yaml file for the model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8edeebc-4373-4543-ad27-94bcbd3dda8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_num = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3d3acdc-da2a-4e9e-b5f1-20b08447eeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'utils.dataloader' from '/home/mushaikh/private/PointCloudPosePrediction/utils/dataloader.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "import os\n",
    "curr_dir = \"/home/mushaikh/private/PointCloudPosePrediction\"\n",
    "# curr_dir = \"/Users/mustafa/Documents/grad school/spring 2024/ECE285 - Visual Learning/PointCloudPosePrediction\"\n",
    "os.chdir(curr_dir)\n",
    "import importlib\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import open3d as o3d\n",
    "from scipy.io import loadmat\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Callable, Optional, Tuple\n",
    "import yaml\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import core.pointnet as pnet\n",
    "import core.deep_pose_estimator as dpe\n",
    "import core.loss as dpe_loss\n",
    "import utils.dataloader\n",
    "import utils.utils\n",
    "import utils.vision\n",
    "importlib.reload(dpe)\n",
    "importlib.reload(pnet)\n",
    "importlib.reload(utils.utils)\n",
    "importlib.reload(utils.vision)\n",
    "importlib.reload(utils.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d98a88de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "else:\n",
    "    torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "608fd5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load params for current model run; choose model run below after creating yaml file for it\n",
    "with open(\"./models/run_{x}/run_{x}.yaml\".format(x=run_num)) as f:\n",
    "    params = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732ecfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader for Eden data - outputs pairs of point clouds and relative poses\n",
    "batch_size = params['train']['batch_size']\n",
    "# \"/Users/mustafa/Documents/grad school/spring 2024/ECE285 - Visual Learning/eden/sample\"\n",
    "dataset = utils.dataloader.EdenDataset(\"/home/mushaikh/private/sample\",\n",
    "                                            keypt_method=params['keypoint_detection']['detection_method'],\n",
    "                                            pcd_num_pts = params['keypoint_detection']['pcd_num_pts']\n",
    "                                            )\n",
    "\n",
    "train_inds, val_inds = torch.utils.data.random_split(dataset, [0.8,0.2])\n",
    "\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(train_inds.indices)\n",
    "valid_sampler = torch.utils.data.SubsetRandomSampler(val_inds.indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10626ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 65/600 [00:40<05:05,  1.75it/s]"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "model = dpe.DeepPoseEstimator(num_pts=params['keypoint_detection']['pcd_num_pts'], \n",
    "                              use_pca = params['model']['use_pca'])\n",
    "model.to(device)\n",
    "lr = params['train']['lr']\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "list_train_loss = []\n",
    "list_val_loss = []\n",
    "\n",
    "for epoch in range(params['train']['epochs']):\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    running_train_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(tqdm(train_loader)):\n",
    "        \n",
    "        # prepare data\n",
    "        pcd1, pcd2, quat, translation = data[\"pcd1\"], data[\"pcd2\"], data[\"quat\"], data[\"translation\"]\n",
    "        pcd1, pcd2 = pcd1.permute(0,2,1), pcd2.permute(0,2,1)\n",
    "        target = torch.cat((translation.transpose(1,0), quat.transpose(1,0))).transpose(1,0)\n",
    "        \n",
    "        # move to gpu\n",
    "        pcd1, pcd2, quat, translation, target = pcd1.to(device), pcd2.to(device), quat.to(device), translation.to(device), target.to(device)\n",
    "        \n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # set mode to train - sets batchnorm/dropout policy etc.\n",
    "        model.train()\n",
    "        \n",
    "        # get preds from model\n",
    "        pred, trans_feat_1, trans_feat_2 = model(pcd1, pcd2) # predicts 7-length vector of translation/quaternion\n",
    "        \n",
    "        # compute loss\n",
    "        loss = dpe_loss.PoseLoss(pred, target) \n",
    "#         + \\\n",
    "#             pnet.feature_transform_regularizer(trans_feat_1) * 0.001 + \\\n",
    "#                 pnet.feature_transform_regularizer(trans_feat_1) * 0.001\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient descent step \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = running_train_loss / (i + 1)\n",
    "    print('Avg. train loss: %f' % (avg_train_loss))\n",
    "    list_train_loss.append(avg_train_loss)\n",
    "    \n",
    "    # update the learning rate scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # check validation loss after every epoch\n",
    "    \n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization\n",
    "    model.eval()\n",
    "    running_vloss = 0.0\n",
    "    \n",
    "    # Disable gradient computation and reduce memory consumption\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            pcd1, pcd2, quat, translation = vdata[\"pcd1\"], vdata[\"pcd2\"], vdata[\"quat\"], vdata[\"translation\"]\n",
    "            pcd1, pcd2 = pcd1.permute(0,2,1), pcd2.permute(0,2,1)\n",
    "            target = torch.cat((translation.transpose(1,0), quat.transpose(1,0))).transpose(1,0)\n",
    "            pcd1, pcd2, quat, translation, target = pcd1.to(device), pcd2.to(device), quat.to(device), translation.to(device), target.to(device)\n",
    "            pred, trans_feat_1, trans_feat_2 = model(pcd1, pcd2)\n",
    "            vloss = dpe_loss.PoseLoss(pred, target) \n",
    "#             + \\\n",
    "#                 pnet.feature_transform_regularizer(trans_feat_1) * 0.001 + \\\n",
    "#                 pnet.feature_transform_regularizer(trans_feat_1) * 0.001\n",
    "            running_vloss += vloss.item()\n",
    "            \n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('Avg. validation loss: {}'.format(avg_vloss))\n",
    "    list_val_loss.append(avg_vloss)\n",
    "    \n",
    "    # save the model after every epoch\n",
    "    torch.save(model.state_dict(), '{y}/run_{x}/run_{x}_deep_pose_{z}.pth'.format(\n",
    "        x=run_num, y=os.path.join(curr_dir,\"models\"), z=epoch))\n",
    "\n",
    "    \n",
    "# plot the loss curves\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(range(params['train']['epochs']), list_train_loss, label=\"train loss\")\n",
    "ax.plot(range(params['train']['epochs']), list_val_loss, label=\"validation loss\")\n",
    "ax.set_xlabel(\"Epoch #\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "fig.legend()\n",
    "fig.savefig(os.path.join(curr_dir,\"results\",\"run_{x}_loss.png\".format(x=run_num)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
